{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Algocrat/simplified-MoR-mvp/blob/main/MoR_Colab_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HJvetJnFK3xm",
      "metadata": {
        "id": "HJvetJnFK3xm"
      },
      "source": [
        "# Small-Data LM Demo (with optional Mixture-of-Recursions)\n",
        "**Goal:** Produce meaningful generations even on a **tiny dataset**, then (optionally) toggle on a simplified **MoR** block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aAZo-e5NK3xo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAZo-e5NK3xo",
        "outputId": "64280987-3caa-4729-e6a3-02dd2e1daa7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.11.13\n",
            "PyTorch: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import sys, torch\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Tip: In Colab, use Runtime → Change runtime type → GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IMeqnsZtK3xp",
      "metadata": {
        "id": "IMeqnsZtK3xp"
      },
      "outputs": [],
      "source": [
        "!pip -q install tiktoken datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aCgbuzlcK3xq",
      "metadata": {
        "id": "aCgbuzlcK3xq"
      },
      "source": [
        "## Data: Tiny Shakespeare (default)\n",
        "This is a ~1MB text file that trains quickly and yields coherent completions with a small model.\n",
        "\n",
        "> You can also try **FineWeb‑Edu (streaming)** later for variety (but Tiny Shakespeare is best for quick wins)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MC0A7LAOK3xq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC0A7LAOK3xq",
        "outputId": "f01a894a-b79e-4a4c-f210-2b47a1091377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Tiny Shakespeare...\n",
            "Chars: 1115394\n",
            "Preview:\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it \n"
          ]
        }
      ],
      "source": [
        "import os, requests, pathlib\n",
        "\n",
        "DATA_DIR = pathlib.Path(\"data\"); DATA_DIR.mkdir(exist_ok=True)\n",
        "TS_PATH = DATA_DIR / \"tiny_shakespeare.txt\"\n",
        "URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "if not TS_PATH.exists():\n",
        "    print(\"Downloading Tiny Shakespeare...\")\n",
        "    r = requests.get(URL, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    TS_PATH.write_text(r.text, encoding=\"utf-8\")\n",
        "else:\n",
        "    print(\"Found existing:\", TS_PATH)\n",
        "\n",
        "text = TS_PATH.read_text(encoding=\"utf-8\")\n",
        "print(\"Chars:\", len(text))\n",
        "print(\"Preview:\\n\", text[:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RaoXsZx7K3xr",
      "metadata": {
        "id": "RaoXsZx7K3xr"
      },
      "source": [
        "### Optional: FineWeb‑Edu (stream just a few rows)\n",
        "Uncomment to sample a few rows and append to the corpus for variety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gX1b5nlQK3xr",
      "metadata": {
        "id": "gX1b5nlQK3xr"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# from itertools import islice\n",
        "# ds_stream = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\", streaming=True)\n",
        "# tiny_rows = list(islice(ds_stream, 50))\n",
        "# text += \"\\n\\n\" + \"\\n\".join([r.get(\"text\",\"\") for r in tiny_rows])\n",
        "# print(\"Augmented corpus length:\", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xtnspgpzK3xr",
      "metadata": {
        "id": "xtnspgpzK3xr"
      },
      "source": [
        "## Tokenizer & dataset builder\n",
        "We use **GPT‑2 BPE** via `tiktoken`. We chunk the token stream into fixed-length sequences for train/val splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sQlDb8jQK3xr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQlDb8jQK3xr",
        "outputId": "8586f677-1338-435b-f993-71347b2efe90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pairs: 3000 | Val pairs: 300 | Vocab≈50257\n"
          ]
        }
      ],
      "source": [
        "import math, random\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")  # 50k BPE\n",
        "ids = enc.encode(text)\n",
        "\n",
        "# Train/val split\n",
        "split = int(0.9 * len(ids))\n",
        "train_ids = ids[:split]\n",
        "val_ids = ids[split:]\n",
        "\n",
        "ctx_len = 256\n",
        "\n",
        "def make_examples(token_ids, num_examples=2000):\n",
        "    examples = []\n",
        "    if len(token_ids) < ctx_len + 1:\n",
        "        return examples\n",
        "    for _ in range(num_examples):\n",
        "        i = random.randint(0, len(token_ids) - ctx_len - 2)\n",
        "        seq = token_ids[i:i+ctx_len+1]\n",
        "        x = torch.tensor(seq[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(seq[1:], dtype=torch.long)\n",
        "        examples.append((x, y))\n",
        "    return examples\n",
        "\n",
        "train_pairs = make_examples(train_ids, num_examples=3000)\n",
        "val_pairs = make_examples(val_ids, num_examples=300)\n",
        "print(f\"Train pairs: {len(train_pairs)} | Val pairs: {len(val_pairs)} | Vocab≈{enc.n_vocab}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KQjISpRAK3xs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQjISpRAK3xs",
        "outputId": "28a633f4-6a01-4588-e234-39d715e1e0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 188 19\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, pairs): self.pairs = pairs\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, i): return self.pairs[i]\n",
        "\n",
        "def collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    return torch.stack(xs), torch.stack(ys)\n",
        "\n",
        "train_loader = DataLoader(PairDataset(train_pairs), batch_size=16, shuffle=True, collate_fn=collate)\n",
        "val_loader   = DataLoader(PairDataset(val_pairs),   batch_size=16, shuffle=False, collate_fn=collate)\n",
        "print(\"Batches:\", len(train_loader), len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vJNbP2EaK3xs",
      "metadata": {
        "id": "vJNbP2EaK3xs"
      },
      "source": [
        "## Small Transformer (baseline)\n",
        "We build a small GPT‑style block with **positional embeddings**, **Multi-Head Attention**, and **SwiGLU** MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wg-uTT2IK3xs",
      "metadata": {
        "id": "wg-uTT2IK3xs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.w = nn.Parameter(torch.ones(d)); self.eps = eps\n",
        "    def forward(self, x): return self.w * x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d, h):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(d, h, bias=False)\n",
        "        self.w2 = nn.Linear(d, h, bias=False)\n",
        "        self.w3 = nn.Linear(h, d, bias=False)\n",
        "    def forward(self, x): return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class MHA(nn.Module):\n",
        "    def __init__(self, d, n_heads):\n",
        "        super().__init__()\n",
        "        assert d % n_heads == 0\n",
        "        self.d, self.h, self.dh = d, n_heads, d // n_heads\n",
        "        self.q = nn.Linear(d, d, bias=False)\n",
        "        self.k = nn.Linear(d, d, bias=False)\n",
        "        self.v = nn.Linear(d, d, bias=False)\n",
        "        self.o = nn.Linear(d, d, bias=False)\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        q = self.q(x).view(B,T,self.h,self.dh).transpose(1,2)\n",
        "        k = self.k(x).view(B,T,self.h,self.dh).transpose(1,2)\n",
        "        v = self.v(x).view(B,T,self.h,self.dh).transpose(1,2)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = y.transpose(1,2).contiguous().view(B,T,D)\n",
        "        return self.o(y)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d, n_heads, mlp_h):\n",
        "        super().__init__()\n",
        "        self.n1 = RMSNorm(d); self.attn = MHA(d, n_heads)\n",
        "        self.n2 = RMSNorm(d); self.mlp  = SwiGLU(d, mlp_h)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.n1(x))\n",
        "        x = x + self.mlp(self.n2(x))\n",
        "        return x\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab, d=256, n_layers=4, n_heads=8, mlp_h=768, max_t=2048):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab, d)\n",
        "        self.pos = nn.Embedding(max_t, d)\n",
        "        self.blocks = nn.ModuleList([Block(d, n_heads, mlp_h) for _ in range(n_layers)])\n",
        "        self.norm = RMSNorm(d)\n",
        "        self.head = nn.Linear(d, vocab, bias=False)\n",
        "    def forward(self, idx):\n",
        "        B,T = idx.shape\n",
        "        x = self.tok(idx) + self.pos(torch.arange(T, device=idx.device)[None,:])\n",
        "        for b in self.blocks: x = b(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BcTEV39_K3xt",
      "metadata": {
        "id": "BcTEV39_K3xt"
      },
      "source": [
        "## Train for a few hundred steps\n",
        "This should already yield **meaningful** completions on Tiny Shakespeare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l1ZUhO9RK3xt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1ZUhO9RK3xt",
        "outputId": "21955532-e376-47c4-90dc-9f2e9f594da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 050  train_loss=1.157  val_loss=8.614\n",
            "step 100  train_loss=0.818  val_loss=9.776\n",
            "step 150  train_loss=0.852  val_loss=9.579\n",
            "step 200  train_loss=0.807  val_loss=9.484\n",
            "step 250  train_loss=0.782  val_loss=9.467\n",
            "step 300  train_loss=0.794  val_loss=9.285\n",
            "step 350  train_loss=0.776  val_loss=9.344\n",
            "step 400  train_loss=0.922  val_loss=9.124\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = TinyGPT(vocab=enc.n_vocab, d=256, n_layers=4, n_heads=8, mlp_h=768).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=500)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    total = 0.0; n = 0\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), label_smoothing=0.05)\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        total += float(loss.detach().cpu()); n += 1\n",
        "    return total / max(1,n)\n",
        "\n",
        "steps = 400\n",
        "for s in range(1, steps+1):\n",
        "    train_loss = run_epoch(train_loader, train=True)\n",
        "    if s % 50 == 0:\n",
        "        val_loss = run_epoch(val_loader, train=False)\n",
        "        print(f\"step {s:03d}  train_loss={train_loss:.3f}  val_loss={val_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19jbdjSzK3xu",
      "metadata": {
        "id": "19jbdjSzK3xu"
      },
      "source": [
        "## Sampling decode (temperature/top‑k + repetition penalty)\n",
        "Use this instead of greedy to avoid repetitive loops on small models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U-E8ZUFHK3xu",
      "metadata": {
        "id": "U-E8ZUFHK3xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3be27b-9ba5-4c11-95cd-bd18ceac32ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "But that are thy true!\n",
            "O, dear love goes hard; and giBear from mine ears,\n",
            "I enter me up blest.\n",
            "\n",
            "FRIARLook, to teach my br marry,door of wilt thou forth thy birth.\n",
            "\n",
            "BENVOLIO:\n",
            "O noble kinsman steeled:\n",
            "O, have made me, nurse!\n",
            "Not so did taste else a mad disdaineth in the sun.\n",
            "Sometimeain that fellow is wings of night\n",
            "To take the thing, could he further sleep for the Capulet,\n",
            "Though sometimes royal\n",
            "----\n",
            "Once upon a time, in Verona, till comes the sun sets oHath\n",
            "Doth say, this is come to die.\n",
            "FRIARENCE:\n",
            "Upon my life, good C prick.\n",
            "\n",
            "ESTER:\n",
            "I know not for my body! belike it well-women:\n",
            "You me much before:\n",
            "To have your hands. I pray you will hold me, excepts,\n",
            "But shall not had a set on thee, than yours.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "And if you suppose join'd for my word;\n",
            "Or delay hath so young of his majesty and\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def sample_next_token(logits, seq_ids, temperature=0.9, top_k=50, repetition_penalty=1.1, penalty_ctx=128):\n",
        "    logits = logits.clone()\n",
        "    if seq_ids is not None and len(seq_ids) > 0:\n",
        "        recent = torch.tensor(seq_ids[-penalty_ctx:], dtype=torch.long, device=logits.device)\n",
        "        logits.index_put_((recent,), logits.index_select(0, recent) / repetition_penalty)\n",
        "    logits = logits / max(1e-6, float(temperature))\n",
        "    if top_k is not None and top_k > 0:\n",
        "        topk_vals, topk_idx = torch.topk(logits, k=min(top_k, logits.numel()))\n",
        "        mask = torch.full_like(logits, float(\"-inf\"))\n",
        "        mask.scatter_(0, topk_idx, topk_vals)\n",
        "        logits = mask\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    return int(torch.multinomial(probs, num_samples=1).item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(model, prompt, max_new_tokens=120, temperature=0.9, top_k=50, repetition_penalty=1.1):\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "    import tiktoken; enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    ids = enc.encode(prompt)\n",
        "    x = torch.tensor(ids, dtype=torch.long, device=dev)[None,:]\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model(x[:, -256:])\n",
        "        next_id = sample_next_token(logits[0, -1], x[0].tolist(), temperature=temperature, top_k=top_k, repetition_penalty=repetition_penalty)\n",
        "        x = torch.cat([x, torch.tensor([[next_id]], device=dev)], dim=1)\n",
        "    return enc.decode(x[0].tolist())\n",
        "\n",
        "print(generate_text(model, \"ROMEO:\", max_new_tokens=120))\n",
        "print(\"----\")\n",
        "print(generate_text(model, \"Once upon a time, in Verona,\", max_new_tokens=120))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f_zBDloLK3xu",
      "metadata": {
        "id": "f_zBDloLK3xu"
      },
      "source": [
        "## (Optional) MoR sanity swap\n",
        "Wrap the last block with a one-recursion MoR wrapper (no routing) to show where MoR would plug in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trMLUnQxK3xu",
      "metadata": {
        "id": "trMLUnQxK3xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35730e6-5559-4278-b2a0-5541fa867967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoR wrapper installed on last block. Try generation again.\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MoRWrapper(nn.Module):\n",
        "    def __init__(self, block: Block):\n",
        "        super().__init__()\n",
        "        self.block = block\n",
        "    def forward(self, x):\n",
        "        # Future work: add expert-choice routing and recursion-wise KV here.\n",
        "        return self.block(x)\n",
        "\n",
        "model.blocks[-1] = MoRWrapper(model.blocks[-1]).to(device)\n",
        "print(\"MoR wrapper installed on last block. Try generation again.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}